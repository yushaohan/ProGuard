<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="ProGuard: Towards Proactive Multimodal Safeguard">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Multimodal Safety Moderation, Reinforcement Learning, Vision-Language Models">
  <!-- TODO: List all authors -->
  <meta name="author" content="Shaohan Yu, Lijun Li, Chenyang Si, Lu Sheng, Jing Shao">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Shanghai Artificial Intelligence Laboratory, PRLab Nanjing University, Beihang University">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="ProGuard: Towards Proactive Multimodal Safeguard">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://yushaohan.github.io/ProGuard/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://yushaohan.github.io/ProGuard/static/images/proguard.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="ProGuard: Towards Proactive Multimodal Safeguard">
  <meta property="article:published_time" content="2025-12-31T00:00:00.000Z">
  <meta property="article:author" content="Shaohan Yu, Lijun Li, Chenyang Si, Lu Sheng, Jing Shao">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Multimodal Safety Moderation">
  <meta property="article:tag" content="Reinforcement Learning">
  <meta property="article:tag" content="Vision-Language Models">

  <!-- Twitter -->
  <meta name="twitter:card" content="">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="">
  <meta name="twitter:image:alt" content="">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="ProGuard: Towards Proactive Multimodal Safeguard">
  <meta name="citation_author" content="Shaohan Yu, Lijun Li, Chenyang Si, Lu Sheng, Jing Shao">
  <meta name="citation_publication_date" content="2025-12-31">
  <meta name="citation_conference_title" content="Arxiv Preprint">
  <meta name="citation_pdf_url" content="">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>ProGuard: Towards Proactive Multimodal Safeguard</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/svg+xml" href="static/images/favicon.svg">
  <link rel="apple-touch-icon" href="static/images/favicon.svg">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "ProGuard: Towards Proactive Multimodal Safeguard",
    "description": "The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.",
    "author": [
      {
        "@type": "Person",
        "name": "Shaohan Yu",
        "affiliation": {
          "@type": "Organization",
          "name": "Shanghai Artificial Intelligence Laboratory, PRLab Nanjing University, Beihang University"
        }
      },
      {
        "@type": "Person",
        "name": "Lijun Li",
        "affiliation": {
          "@type": "Organization",
          "name": "Shanghai Artificial Intelligence Laboratory"
        }
      },
      {
        "@type": "Person",
        "name": "Chenyang Si",
        "affiliation": {
          "@type": "Organization",
          "name": "PRLab Nanjing University"
        }
      },
      {
        "@type": "Person",
        "name": "Lu Sheng",
        "affiliation": {
          "@type": "Organization",
          "name": "Beihang University"
        }
      },
      {
        "@type": "Person",
        "name": "Jing Shao",
        "affiliation": {
          "@type": "Organization",
          "name": "Shanghai Artificial Intelligence Laboratory"
        }
      }
    ],
    "datePublished": "2025-12-31",
    "publisher": {
      "@type": "Organization",
      "name": "Arxiv Preprint"
    },
    "url": "https://yushaohan.github.io/ProGuard/",
    "image": "https://yushaohan.github.io/ProGuard/static/images/proguard.png",
    "keywords": ["Multimodal Safety Moderation", "Reinforcement Learning", "Vision-Language Models"],
    "abstract": "The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.",
    "citation": "",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://yushaohan.github.io/ProGuard/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Multimodal Safety Moderation"
      },
      {
        "@type": "Thing", 
        "name": "Reinforcement Learning"
      },
      {
        "@type": "Thing",
        "name": "Vision-Language Models"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Shanghai Artificial Intelligence Laboratory, PRLab Nanjing University, Beihang University",
    "url": "https://yushaohan.github.io/ProGuard/",
    "logo": "https://yushaohan.github.io/ProGuard/static/images/proguard.png",
    "sameAs": []
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">ProGuard: Towards Proactive Multimodal Safeguard</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://yushaohan.github.io/" target="_blank">Shaohan Yu</a><sup>1,2,3*</sup>,</span>
              <span class="author-block">
                <a href="https://adwardlee.github.io/" target="_blank">Lijun Li</a><sup>1*‚Ä†</sup>,</span>
              <span class="author-block">
                <a href="https://chenyangsi.top/" target="_blank">Chenyang Si</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://lucassheng.github.io/" target="_blank">Lu Sheng</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://amandajshao.github.io/" target="_blank">Jing Shao</a><sup>1‚Ä†</sup>
              </span>
            </div>

              <div class="is-size-5 publication-authors">
                <!-- TODO: Replace with your institution and conference/journal info -->
                <span class="author-block">
                  <sup>1</sup>Shanghai Artificial Intelligence Laboratory, 
                  <sup>2</sup>PRLab, Nanjing University, 
                  <sup>3</sup>Beihang University
                </span>
                <!-- TODO: Remove this line if no equal contribution -->
                <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution,<sup>‚Ä†</sup>Corresponding Author</small></span>
              </div>

              <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                    <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span>üìÑ Paper (Coming Soon)</span>
                      </a>
                    </span>

                  <span class="link-block">
                    <a href="https://github.com/yushaohan/ProGuard" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span>‚öôÔ∏è Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://huggingface.co/collections/yushaohan/proguard" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span>ü§ó Model & Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Intro image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/intro.png" alt="Introduction" style="width: 60%; height: auto; display: block; margin: 0 auto;">
      <h2 class="subtitle has-text-centered" style="margin-top: 2rem;">
        Comparison of moderation workflows between <strong>Reactive Guard</strong> (left) and <strong>Proactive Guard</strong> (right). After determining safety, Reactive Guard can only perform multi-class classification of safety risks based on a provided static taxonomy. In contrast, Proactive Guard first determines whether existing safety risks belong to known categories in the static taxonomy. If not, it infers reasonable safety category names. Therefore, Proactive Guard not only possesses the ability to <strong>understand</strong> safety policies but also <strong>generates</strong> reasonable safety category descriptions.
      </h2>
    </div>
  </div>
</section>
<!-- End intro image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- ProGuard overview image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/proguard.png" alt="ProGuard Overview" style="width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        Overview of ProGuard. The framework combines moderation task design, a multimodal safety taxonomy and a balanced safety dataset, and an online reinforcement learning pipeline to build a reasoning-enhanced proactive guard model.
      </h2>
    </div>
  </div>
</section>
<!-- End ProGuard overview image -->

<!-- Table 1 -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/table1.png" alt="Table 1" style="width: 100%; height: auto;">
      <h2 class="subtitle" style="text-align: left; margin-top: 1.5rem;">
        On the Binary Safety Classification task, results measured using F1-Score demonstrate that ProGuard achieves performance comparable to closed-source large model APIs with fewer parameters.
      </h2>
    </div>
  </div>
</section>
<!-- End Table 1 -->

<!-- Table 2 -->
<section class="hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/table2.png" alt="Table 2" style="width: 100%; height: auto;">
      <h2 class="subtitle" style="text-align: left; margin-top: 1.5rem;">
        On the Unsafe Content Categorization task, results measured using Accuracy show that ProGuard outperforms all open-source guard models, though there remains a performance gap compared to closed-source APIs.
      </h2>
    </div>
  </div>
</section>
<!-- End Table 2 -->

<!-- Table 3 -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/table3.png" alt="Table 3" style="width: 100%; height: auto;">
      <h2 class="subtitle" style="text-align: left; margin-top: 1.5rem;">
        On the OOD safety category inference task, models must first determine whether a dialogue belongs to any category in the static safety taxonomy. Results measured using F1-score show that ProGuard outperforms closed-source large models on in/out-of-taxonomy classification.
      </h2>
    </div>
  </div>
</section>
<!-- End Table 3 -->

<!-- Table 4 -->
<section class="hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/table4.png" alt="Table 4" style="width: 100%; height: auto;">
      <h2 class="subtitle" style="text-align: left; margin-top: 1.5rem;">
        On the OOD safety category inference task, models need to provide a safety category inference when the input is out of taxonomy. Evaluation using the proposed similarity-based reward reveals that ProGuard's performance approaches that of closed-source large models.
      </h2>
    </div>
  </div>
</section>
<!-- End Table 4 -->






<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{YourPaperKey2024,
  title={Your Paper Title Here},
  author={First Author and Second Author and Third Author},
  journal={Conference/Journal Name},
  year={2024},
  url={https://your-domain.com/your-project-page}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
